defaults:
  - GradDiff

handler: RMU
method_args:
  # The params here are more dependent on model and dataset. Tune them carefully to work
  gamma: 1.0
  steering_coeff: 2
  retain_loss_type: EMBED_DIFF
  alpha: 1 
  module_regex: model\.layers\.7
  trainable_params_regex: 
    - .* # update all parameters
    # - model\.layers\.(5|6|7)\.mlp\.down_proj\.weight # If you want to update only these weights