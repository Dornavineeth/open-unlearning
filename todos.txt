can link major survey papers and awesome-llm-unlearning as well

link all papers implemented

refactor forget quality and truth ratio

python src/eval.py --config-name=eval.yaml experiment=eval/muse/default task_name=eval_MIA

PR formatting improvements
minor docs fixes and changes
add contributing.md: link to the other docs in it

check other changes how to introduce


add details of leaderboard
    steps to add new result to leaderboard (exp, results)
    inform about hyperparameter tuning (in the experiments config yaml file)
    give one example with PLACEHOLDERS
        if using different DS config, explain
    scripts/contributions/<paper_name>/
        ds/accelerate config (if any)
        repro.md file (links papers, talk about hyperparameters, results)
            need template
            different template for benchmark and method
        run.sh (should exactly run the method on whatever baselines are supported)
    results.md will be updated with best hyperparams
    point to experiments.md
    * mark the previous results and say they are untuned
    citations file

re-contact and reply to bo yang after documentation update

# inputs to the metric are: (see memorization.py for example of usage)
# model
# kwargs
#     pre_compute: dictionary containing prior metrics needed for current metric computation
#     batch_size, generation_args: generic setting values
#     data: the torch dataset
#     tokenizer: the loaded object
#     collator: the loaded object
#     metric specific args like hyperparams - including rouge_type, aggregator

eval.tofu.overwrite=true

model=Llama-2-7b-hf
data_split=News
python src/eval.py --config-name=eval.yaml experiment=eval/muse/default task_name=eval_MIA data_split=${data_split} retain_logs_path=saves/eval/muse_${model}_${data_split}_retrain/MUSE_EVAL.json eval.muse.overwrite=true eval.muse.metrics.privleak.pre_compute.mia.handler=mia_min_k


data_split=Books
python src/eval.py --config-name=eval.yaml experiment=eval/muse/default task_name=eval_MIA data_split=${data_split} retain_logs_path=saves/eval/muse_${model}_${data_split}_retrain/MUSE_EVAL.json eval.muse.overwrite=true eval.muse.metrics.privleak.pre_compute.mia.handler=mia_min_k_plus_plus

data_split=News
python src/eval.py --config-name=eval.yaml experiment=eval/muse/default task_name=eval_MIA_2 data_split=News retain_logs_path=saves/eval/muse_${model}_${data_split}_retrain/MUSE_EVAL.json eval.muse.overwrite=true eval.muse.metrics.privleak.pre_compute.mia.handler=mia_min_k_plus_plus


python src/eval.py --config-name=eval.yaml experiment=eval/muse/default task_name=eval_MIA_0 data_split=Books retain_logs_path=saves/eval/muse_${model}_${data_split}_retrain/MUSE_EVAL.json eval.muse.overwrite=true eval.muse.metrics.privleak.pre_compute.mia.handler=mia_reference

python src/eval.py --config-name=eval.yaml experiment=eval/muse/default task_name=eval_MIA_1 data_split=Books retain_logs_path=saves/eval/muse_${model}_${data_split}_retrain/MUSE_EVAL.json eval.muse.overwrite=true eval.muse.metrics.privleak.pre_compute.mia.handler=mia_loss

python src/eval.py --config-name=eval.yaml experiment=eval/muse/default task_name=eval_MIA_6 data_split=Books retain_logs_path=saves/eval/muse_${model}_${data_split}_retrain/MUSE_EVAL.json eval.muse.overwrite=true eval.muse.metrics.privleak.pre_compute.mia.handler=mia_gradnorm

python src/eval.py --config-name=eval.yaml experiment=eval/muse/default task_name=eval_MIA_5 data_split=Books retain_logs_path=saves/eval/muse_${model}_${data_split}_retrain/MUSE_EVAL.json eval.muse.overwrite=true eval.muse.metrics.privleak.pre_compute.mia.handler=mia_zlib

tested:
minK
minK++
loss

zlib
gradnorm -- test on TOFU
reference -- running